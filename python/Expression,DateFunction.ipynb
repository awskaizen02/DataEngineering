{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c81851f-9474-474e-b092-76a04aead8b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Expression language : whenever a pyspark funcation is not found, we can use SQL statement inside a pyspark stetement using the expression language\n",
    "#syntax: expr(\"SQL STATEMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f959afd-f172-44ea-b875-cec1a89587ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+\n|firstname|lastname|country|state|  sal|\n+---------+--------+-------+-----+-----+\n|     raja|  pushpa|    USA|     |30000|\n|    priya|  pushpa|    USA|     |29900|\n|  Karthik|    Subu|    USA|   CA| 6000|\n|    James|   Smith|    USA|   FL|20000|\n|   Martin|   Jones|    USA|   CA| 3000|\n|      Sam|Anderson|     UK|  LND| 8000|\n|    Maria| Patrick|     UK|  MCR| 7000|\n|    Robet|   Bevon|     UK|  LND| 3500|\n|    Maria|Anderson|     UK|  MCR| 3000|\n+---------+--------+-------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "staticlist = [(\" raja\", \"pushpa\", \"USA\",\"\",30000),\n",
    "            (\" priya\", \"pushpa\", \"USA\",\"\",29900),\n",
    "            (\" Karthik\", \"Subu\", \"USA\",\"CA\",6000),\n",
    "            (\" James\", \"Smith\", \"USA\",\"FL\",20000),\n",
    "            (\"Martin\", \"Jones\", \"USA\",\"CA\",3000),\n",
    "            (\"Sam\", \"Anderson\", \"UK\",\"LND\",8000),\n",
    "            (\"Maria\", \"Patrick\", \"UK\",\"MCR\",7000),\n",
    "            (\"Robet\", \"Bevon\", \"UK\",\"LND\",3500),\n",
    "            (\"Maria\", \"Anderson\", \"UK\",\"MCR\",3000)\n",
    "            ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\",\"sal\"]            \n",
    "df = spark.createDataFrame( data = staticlist, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b114d3-1831-4021-9595-d070a479325e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+---------------+\n|firstname|lastname|country|state|  sal|Updated_Country|\n+---------+--------+-------+-----+-----+---------------+\n|     raja|  pushpa|    USA|     |30000|  UNITED STATES|\n|    priya|  pushpa|    USA|     |29900|  UNITED STATES|\n|  Karthik|    Subu|    USA|   CA| 6000|  UNITED STATES|\n|    James|   Smith|    USA|   FL|20000|  UNITED STATES|\n|   Martin|   Jones|    USA|   CA| 3000|  UNITED STATES|\n|      Sam|Anderson|     UK|  LND| 8000|             UK|\n|    Maria| Patrick|     UK|  MCR| 7000|             UK|\n|    Robet|   Bevon|     UK|  LND| 3500|             UK|\n|    Maria|Anderson|     UK|  MCR| 3000|             UK|\n+---------+--------+-------+-----+-----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# REPLACE USA WITH UNITTED STATES\n",
    "from pyspark.sql.functions import *\n",
    "df2 = df.withColumn(\"Updated_Country\",expr(\"case when country = 'USA' then 'UNITED STATES' else country end\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce76393-e9c9-4634-b9c8-c99fa67ca682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+-------------+\n|firstname|lastname|country|state|  sal|     Up_Cntry|\n+---------+--------+-------+-----+-----+-------------+\n|     raja|  pushpa|    USA|     |30000|UNITED STATES|\n|    priya|  pushpa|    USA|     |29900|UNITED STATES|\n|  Karthik|    Subu|    USA|   CA| 6000|UNITED STATES|\n|    James|   Smith|    USA|   FL|20000|UNITED STATES|\n|   Martin|   Jones|    USA|   CA| 3000|UNITED STATES|\n|      Sam|Anderson|     UK|  LND| 8000|           UK|\n|    Maria| Patrick|     UK|  MCR| 7000|           UK|\n|    Robet|   Bevon|     UK|  LND| 3500|           UK|\n|    Maria|Anderson|     UK|  MCR| 3000|           UK|\n+---------+--------+-------+-----+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"Up_Cntry\", expr(\"replace(country,'USA','UNITED STATES')\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f345ef66-07e4-4e51-a742-e76fbac2e4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DATE USECASES\n",
    "# YTD, MTD, QTD,WTD,MOM,YOY,YEAR,QUARTER, MONTH, WEEK, PREVIOUS YEAR, PREVIOUS MONTH, CONVERT TO DATE\n",
    "# LAST 12 MONTH ENDS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7887c7-0067-4e20-8191-3128737234ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+\n|S_no| StartDate|   EndDate|\n+----+----------+----------+\n|   1|2023-06-07|12/31/2023|\n|   2|2022-02-01|11/30/2022|\n|   3|2023-12-31|11/30/2023|\n|   4|2024-01-01|01/31/2024|\n|   5|2024-02-01|02/28/2024|\n|   6|2025-01-07|06/30/2025|\n+----+----------+----------+\n\nroot\n |-- S_no: long (nullable = true)\n |-- StartDate: string (nullable = true)\n |-- EndDate: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "data = [(1,'2023-06-07','12/31/2023'),\n",
    "        (2,'2022-02-01','11/30/2022'),\n",
    "        (3,'2023-12-31','11/30/2023'),\n",
    "        (4,'2024-01-01','01/31/2024'),\n",
    "        (5,'2024-02-01','02/28/2024'),\n",
    "        (6,'2025-01-07','06/30/2025')]\n",
    "col = ['S_no','StartDate','EndDate']\n",
    "df = spark.createDataFrame(data = data, schema = col)        \n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e188a0-cf96-46be-b2ec-0e48b735d872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+\n|S_no| StartDate|EndDate|\n+----+----------+-------+\n|   1|2023-06-07|   null|\n|   2|2022-02-01|   null|\n|   3|2023-12-31|   null|\n|   4|2024-01-01|   null|\n|   5|2024-02-01|   null|\n|   6|2025-01-07|   null|\n+----+----------+-------+\n\nroot\n |-- S_no: long (nullable = true)\n |-- StartDate: date (nullable = true)\n |-- EndDate: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Cast a string data type to data type\n",
    "# cast to a date will return a value if the string is in yyyy-mm-dd format. Any other format , it will retrun null.\n",
    "# use to_date function to handle other format scenarios\n",
    "\n",
    "df_cast = df.withColumn(\"StartDate\",df.StartDate.cast(DateType()))\\\n",
    "            .withColumn(\"EndDate\",df.EndDate.cast(DateType()))\n",
    "df_cast.show()   \n",
    "df_cast.printSchema()         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c791e1d3-2cae-4253-ae2d-479f3fd56f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+\n|S_no| StartDate|   EndDate|\n+----+----------+----------+\n|   1|2023-06-07|2023-12-31|\n|   2|2022-02-01|2022-11-30|\n|   3|2023-12-31|2023-11-30|\n|   4|2024-01-01|2024-01-31|\n|   5|2024-02-01|2024-02-28|\n|   6|2025-01-07|2025-06-30|\n+----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# to hanle above scenariom use to_date function\n",
    "# use todate to convert the string into a date format\n",
    "# syntax: to_date(\"DATECOL\",\"dateformat\")\n",
    "# to_date doesnt require any optional sting format if the current string is in yyyy-mm-dd format. if the sting  # date is any other format, optional string is mandatory\n",
    "\n",
    "df_cast = df.withColumn(\"StartDate\",df.StartDate.cast(DateType()))\\\n",
    "            .withColumn(\"EndDate\",to_date(\"EndDate\",'MM/dd/yyyy'))\n",
    "df_cast.show()            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e236fb41-6d57-4427-8e29-4d6792a510ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----+-------+-----+---------+--------+--------+\n|S_no| StartDate|   EndDate|YEAR|QUARTER|MONTH|DAY_MONTH|DAY_YEAR|DAY_WEEK|\n+----+----------+----------+----+-------+-----+---------+--------+--------+\n|   1|2023-06-07|12/31/2023|2023|      2|    6|        7|     158|       4|\n|   2|2022-02-01|11/30/2022|2022|      1|    2|        1|      32|       3|\n|   3|2023-12-31|11/30/2023|2023|      4|   12|       31|     365|       1|\n|   4|2024-01-01|01/31/2024|2024|      1|    1|        1|       1|       2|\n|   5|2024-02-01|02/28/2024|2024|      1|    2|        1|      32|       5|\n|   6|2025-01-07|06/30/2025|2025|      1|    1|        7|       7|       3|\n+----+----------+----------+----+-------+-----+---------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#GET YEAR, MONTH, QUARTER, DAY\n",
    "\n",
    "df_dates = df.withColumn(\"YEAR\",year(\"StartDate\"))\\\n",
    "             .withColumn(\"QUARTER\",quarter(\"StartDate\"))\\\n",
    "              .withColumn(\"MONTH\",month(\"StartDate\"))\\\n",
    "              .withColumn(\"DAY_MONTH\",dayofmonth(\"StartDate\"))\\\n",
    "              .withColumn(\"DAY_YEAR\",dayofyear(\"StartDate\"))\\\n",
    "              .withColumn(\"DAY_WEEK\",dayofweek(\"StartDate\"))\n",
    "df_dates.show()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94bab3be-0938-49d4-addc-df60974ed543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+\n|S_no| StartDate|   EndDate| ADD2_Days|\n+----+----------+----------+----------+\n|   1|2023-06-07|12/31/2023|2023-06-09|\n|   2|2022-02-01|11/30/2022|2022-02-03|\n|   3|2023-12-31|11/30/2023|2024-01-02|\n|   4|2024-01-01|01/31/2024|2024-01-03|\n|   5|2024-02-01|02/28/2024|2024-02-03|\n|   6|2025-01-07|06/30/2025|2025-01-09|\n+----+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#ADD DATE TO EXISTING DATES\n",
    "df_add1 = df.withColumn(\"ADD2_Days\",date_add(\"StartDate\",2))\n",
    "df_add1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ae6d6f-899f-44a8-a103-b5e95ebd2a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+\n|S_no| StartDate|   EndDate|SUB_2_days|\n+----+----------+----------+----------+\n|   1|2023-06-07|12/31/2023|2023-06-05|\n|   2|2022-02-01|11/30/2022|2022-01-30|\n|   3|2023-12-31|11/30/2023|2023-12-29|\n|   4|2024-01-01|01/31/2024|2023-12-30|\n|   5|2024-02-01|02/28/2024|2024-01-30|\n|   6|2025-01-07|06/30/2025|2025-01-05|\n+----+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#REMOVE DAYS FROM  EXISTING DATE\n",
    "df_rem = df.withColumn(\"SUB_2_days\",date_sub(\"StartDate\",2))\n",
    "df_rem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3138c5a-59e0-4c9e-a6df-c2317659ccdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+------------+---------------+\n|S_no| StartDate|   EndDate|add_2_months|Remove_2_months|\n+----+----------+----------+------------+---------------+\n|   1|2023-06-07|12/31/2023|  2023-08-07|     2023-04-07|\n|   2|2022-02-01|11/30/2022|  2022-04-01|     2021-12-01|\n|   3|2023-12-31|11/30/2023|  2024-02-29|     2023-10-31|\n|   4|2024-01-01|01/31/2024|  2024-03-01|     2023-11-01|\n|   5|2024-02-01|02/28/2024|  2024-04-01|     2023-12-01|\n|   6|2025-01-07|06/30/2025|  2025-03-07|     2024-11-07|\n+----+----------+----------+------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#ADD/REMOVE moths to existing date\n",
    "# FOR YEAR, QUARTER,HALF YEAR use multiples of monts as we dont have a direct dataadd funcion for year, quarter,month\n",
    "#2 TEAR = 24 MONTHS, 2 QUARTERS = 6 MONTHS\n",
    "\n",
    "df_add2 = df.withColumn(\"add_2_months\",add_months(\"StartDate\",2))\\\n",
    "            .withColumn(\"Remove_2_months\",add_months(\"StartDate\",-2))\n",
    "df_add2.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7420a3ca-1a9d-433d-b47f-6f7d447f30d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------------+-------------------+-------------------+\n|S_no| StartDate|   EndDate|        ADD_2_years|     ADD_3_Quarters|        ADD_5_weeks|\n+----+----------+----------+-------------------+-------------------+-------------------+\n|   1|2023-06-07|12/31/2023|2025-06-07 00:00:00|2024-03-07 00:00:00|2023-07-12 00:00:00|\n|   2|2022-02-01|11/30/2022|2024-02-01 00:00:00|2022-11-01 00:00:00|2022-03-08 00:00:00|\n|   3|2023-12-31|11/30/2023|2025-12-31 00:00:00|2024-09-30 00:00:00|2024-02-04 00:00:00|\n|   4|2024-01-01|01/31/2024|2026-01-01 00:00:00|2024-10-01 00:00:00|2024-02-05 00:00:00|\n|   5|2024-02-01|02/28/2024|2026-02-01 00:00:00|2024-11-01 00:00:00|2024-03-07 00:00:00|\n|   6|2025-01-07|06/30/2025|2027-01-07 00:00:00|2025-10-07 00:00:00|2025-02-11 00:00:00|\n+----+----------+----------+-------------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Alternate option to use date add for year, quarter,week\n",
    "# use EXPRESSION LANUAGE\n",
    "df_add3 = df.withColumn(\"ADD_2_years\",expr(\"DATEADD(year,2,StartDate)\"))\\\n",
    "            .withColumn(\"ADD_3_Quarters\",expr(\"DATEADD(quarter,3,StartDate)\"))\\\n",
    "            .withColumn(\"ADD_5_weeks\",expr(\"DATEADD(WEEK,5,sTARTdATE)\"))    \n",
    "df_add3.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a754dd7-e8df-40ad-99c7-ca6ea4fbeee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+\n|S_no| StartDate|   EndDate|  Last_Day|\n+----+----------+----------+----------+\n|   1|2023-06-07|12/31/2023|2023-06-30|\n|   2|2022-02-01|11/30/2022|2022-02-28|\n|   3|2023-12-31|11/30/2023|2023-12-31|\n|   4|2024-01-01|01/31/2024|2024-01-31|\n|   5|2024-02-01|02/28/2024|2024-02-29|\n|   6|2025-01-07|06/30/2025|2025-01-31|\n+----+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#LAST DAY OF MONTH\n",
    "df_lst = df.withColumn(\"Last_Day\",last_day(\"StartDate\"))\n",
    "df_lst.show()                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf51d35a-ed6b-47ef-8f00-11b827f239d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-----------+--------------------+\n|S_no| StartDate|   EndDate|CurrentDate|    CUrrentTimeStamp|\n+----+----------+----------+-----------+--------------------+\n|   1|2023-06-07|12/31/2023| 2025-06-24|2025-06-24 16:47:...|\n|   2|2022-02-01|11/30/2022| 2025-06-24|2025-06-24 16:47:...|\n|   3|2023-12-31|11/30/2023| 2025-06-24|2025-06-24 16:47:...|\n|   4|2024-01-01|01/31/2024| 2025-06-24|2025-06-24 16:47:...|\n|   5|2024-02-01|02/28/2024| 2025-06-24|2025-06-24 16:47:...|\n|   6|2025-01-07|06/30/2025| 2025-06-24|2025-06-24 16:47:...|\n+----+----------+----------+-----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Currentdate will give the output as date only\n",
    "#Currenttimestamp will return both date and time in UTC timezone\n",
    "\n",
    "df_curr = df.withColumn(\"CurrentDate\",current_date())\\\n",
    "            .withColumn(\"CUrrentTimeStamp\",current_timestamp())\n",
    "df_curr.show()            "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-06-24 21:20:48",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}