{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c81851f-9474-474e-b092-76a04aead8b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Expression language : whenever a pyspark funcation is not found, we can use SQL statement inside a pyspark stetement using the expression language\n",
    "#syntax: expr(\"SQL STATEMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f959afd-f172-44ea-b875-cec1a89587ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+\n|firstname|lastname|country|state|  sal|\n+---------+--------+-------+-----+-----+\n|     raja|  pushpa|    USA|     |30000|\n|    priya|  pushpa|    USA|     |29900|\n|  Karthik|    Subu|    USA|   CA| 6000|\n|    James|   Smith|    USA|   FL|20000|\n|   Martin|   Jones|    USA|   CA| 3000|\n|      Sam|Anderson|     UK|  LND| 8000|\n|    Maria| Patrick|     UK|  MCR| 7000|\n|    Robet|   Bevon|     UK|  LND| 3500|\n|    Maria|Anderson|     UK|  MCR| 3000|\n+---------+--------+-------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "staticlist = [(\" raja\", \"pushpa\", \"USA\",\"\",30000),\n",
    "            (\" priya\", \"pushpa\", \"USA\",\"\",29900),\n",
    "            (\" Karthik\", \"Subu\", \"USA\",\"CA\",6000),\n",
    "            (\" James\", \"Smith\", \"USA\",\"FL\",20000),\n",
    "            (\"Martin\", \"Jones\", \"USA\",\"CA\",3000),\n",
    "            (\"Sam\", \"Anderson\", \"UK\",\"LND\",8000),\n",
    "            (\"Maria\", \"Patrick\", \"UK\",\"MCR\",7000),\n",
    "            (\"Robet\", \"Bevon\", \"UK\",\"LND\",3500),\n",
    "            (\"Maria\", \"Anderson\", \"UK\",\"MCR\",3000)\n",
    "            ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\",\"sal\"]            \n",
    "df = spark.createDataFrame( data = staticlist, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b114d3-1831-4021-9595-d070a479325e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+---------------+\n|firstname|lastname|country|state|  sal|Updated_Country|\n+---------+--------+-------+-----+-----+---------------+\n|     raja|  pushpa|    USA|     |30000|  UNITED STATES|\n|    priya|  pushpa|    USA|     |29900|  UNITED STATES|\n|  Karthik|    Subu|    USA|   CA| 6000|  UNITED STATES|\n|    James|   Smith|    USA|   FL|20000|  UNITED STATES|\n|   Martin|   Jones|    USA|   CA| 3000|  UNITED STATES|\n|      Sam|Anderson|     UK|  LND| 8000|             UK|\n|    Maria| Patrick|     UK|  MCR| 7000|             UK|\n|    Robet|   Bevon|     UK|  LND| 3500|             UK|\n|    Maria|Anderson|     UK|  MCR| 3000|             UK|\n+---------+--------+-------+-----+-----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# REPLACE USA WITH UNITTED STATES\n",
    "from pyspark.sql.functions import *\n",
    "df2 = df.withColumn(\"Updated_Country\",expr(\"case when country = 'USA' then 'UNITED STATES' else country end\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce76393-e9c9-4634-b9c8-c99fa67ca682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+-------------+\n|firstname|lastname|country|state|  sal|     Up_Cntry|\n+---------+--------+-------+-----+-----+-------------+\n|     raja|  pushpa|    USA|     |30000|UNITED STATES|\n|    priya|  pushpa|    USA|     |29900|UNITED STATES|\n|  Karthik|    Subu|    USA|   CA| 6000|UNITED STATES|\n|    James|   Smith|    USA|   FL|20000|UNITED STATES|\n|   Martin|   Jones|    USA|   CA| 3000|UNITED STATES|\n|      Sam|Anderson|     UK|  LND| 8000|           UK|\n|    Maria| Patrick|     UK|  MCR| 7000|           UK|\n|    Robet|   Bevon|     UK|  LND| 3500|           UK|\n|    Maria|Anderson|     UK|  MCR| 3000|           UK|\n+---------+--------+-------+-----+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"Up_Cntry\", expr(\"replace(country,'USA','UNITED STATES')\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f345ef66-07e4-4e51-a742-e76fbac2e4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DATE USECASES\n",
    "# YTD, MTD, QTD,WTD,MOM,YOY,YEAR,QUARTER, MONTH, WEEK, PREVIOUS YEAR, PREVIOUS MONTH, CONVERT TO DATE\n",
    "# LAST 12 MONTH ENDS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7887c7-0067-4e20-8191-3128737234ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+\n|S_no| StartDate|   EndDate|\n+----+----------+----------+\n|   1|2023-06-07|12/31/2023|\n|   2|2022-02-01|11/30/2022|\n|   3|2023-12-31|11/30/2023|\n|   4|2024-01-01|01/31/2024|\n|   5|2024-02-01|02/28/2024|\n|   6|2025-01-07|06/30/2025|\n+----+----------+----------+\n\nroot\n |-- S_no: long (nullable = true)\n |-- StartDate: string (nullable = true)\n |-- EndDate: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "data = [(1,'2023-06-07','12/31/2023'),\n",
    "        (2,'2022-02-01','11/30/2022'),\n",
    "        (3,'2023-12-31','11/30/2023'),\n",
    "        (4,'2024-01-01','01/31/2024'),\n",
    "        (5,'2024-02-01','02/28/2024'),\n",
    "        (6,'2025-01-07','06/30/2025')]\n",
    "col = ['S_no','StartDate','EndDate']\n",
    "df = spark.createDataFrame(data = data, schema = col)        \n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e188a0-cf96-46be-b2ec-0e48b735d872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+\n|S_no| StartDate|EndDate|\n+----+----------+-------+\n|   1|2023-06-07|   null|\n|   2|2022-02-01|   null|\n|   3|2023-12-31|   null|\n|   4|2024-01-01|   null|\n|   5|2024-02-01|   null|\n|   6|2025-01-07|   null|\n+----+----------+-------+\n\nroot\n |-- S_no: long (nullable = true)\n |-- StartDate: date (nullable = true)\n |-- EndDate: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Cast a string data type to data type\n",
    "# cast to a date will return a value if the string is in yyyy-mm-dd format. Any other format , it will retrun null.\n",
    "# use to_date function to handle other format scenarios\n",
    "\n",
    "df_cast = df.withColumn(\"StartDate\",df.StartDate.cast(DateType()))\\\n",
    "            .withColumn(\"EndDate\",df.EndDate.cast(DateType()))\n",
    "df_cast.show()   \n",
    "df_cast.printSchema()         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c791e1d3-2cae-4253-ae2d-479f3fd56f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+\n|S_no| StartDate|   EndDate|\n+----+----------+----------+\n|   1|2023-06-07|2023-12-31|\n|   2|2022-02-01|2022-11-30|\n|   3|2023-12-31|2023-11-30|\n|   4|2024-01-01|2024-01-31|\n|   5|2024-02-01|2024-02-28|\n|   6|2025-01-07|2025-06-30|\n+----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# to hanle above scenariom use to_date function\n",
    "# use todate to convert the string into a date format\n",
    "# syntax: to_date(\"DATECOL\",\"dateformat\")\n",
    "# to_date doesnt require any optional sting format if the current string is in yyyy-mm-dd format. if the sting  # date is any other format, optional string is mandatory\n",
    "\n",
    "df_cast = df.withColumn(\"StartDate\",df.StartDate.cast(DateType()))\\\n",
    "            .withColumn(\"EndDate\",to_date(\"EndDate\",'MM/dd/yyyy'))\n",
    "df_cast.show()            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e236fb41-6d57-4427-8e29-4d6792a510ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----+-------+-----+---------+--------+--------+\n|S_no| StartDate|   EndDate|YEAR|QUARTER|MONTH|DAY_MONTH|DAY_YEAR|DAY_WEEK|\n+----+----------+----------+----+-------+-----+---------+--------+--------+\n|   1|2023-06-07|12/31/2023|2023|      2|    6|        7|     158|       4|\n|   2|2022-02-01|11/30/2022|2022|      1|    2|        1|      32|       3|\n|   3|2023-12-31|11/30/2023|2023|      4|   12|       31|     365|       1|\n|   4|2024-01-01|01/31/2024|2024|      1|    1|        1|       1|       2|\n|   5|2024-02-01|02/28/2024|2024|      1|    2|        1|      32|       5|\n|   6|2025-01-07|06/30/2025|2025|      1|    1|        7|       7|       3|\n+----+----------+----------+----+-------+-----+---------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#GET YEAR, MONTH, QUARTER, DAY\n",
    "\n",
    "df_dates = df.withColumn(\"YEAR\",year(\"StartDate\"))\\\n",
    "             .withColumn(\"QUARTER\",quarter(\"StartDate\"))\\\n",
    "              .withColumn(\"MONTH\",month(\"StartDate\"))\\\n",
    "              .withColumn(\"DAY_MONTH\",dayofmonth(\"StartDate\"))\\\n",
    "              .withColumn(\"DAY_YEAR\",dayofyear(\"StartDate\"))\\\n",
    "              .withColumn(\"DAY_WEEK\",dayofweek(\"StartDate\"))\n",
    "df_dates.show()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94bab3be-0938-49d4-addc-df60974ed543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+\n|S_no| StartDate|   EndDate| ADD2_Days|\n+----+----------+----------+----------+\n|   1|2023-06-07|12/31/2023|2023-06-09|\n|   2|2022-02-01|11/30/2022|2022-02-03|\n|   3|2023-12-31|11/30/2023|2024-01-02|\n|   4|2024-01-01|01/31/2024|2024-01-03|\n|   5|2024-02-01|02/28/2024|2024-02-03|\n|   6|2025-01-07|06/30/2025|2025-01-09|\n+----+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#ADD DATE TO EXISTING DATES\n",
    "df_add1 = df.withColumn(\"ADD2_Days\",date_add(\"StartDate\",2))\n",
    "df_add1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ae6d6f-899f-44a8-a103-b5e95ebd2a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+\n|S_no| StartDate|   EndDate|SUB_2_days|\n+----+----------+----------+----------+\n|   1|2023-06-07|12/31/2023|2023-06-05|\n|   2|2022-02-01|11/30/2022|2022-01-30|\n|   3|2023-12-31|11/30/2023|2023-12-29|\n|   4|2024-01-01|01/31/2024|2023-12-30|\n|   5|2024-02-01|02/28/2024|2024-01-30|\n|   6|2025-01-07|06/30/2025|2025-01-05|\n+----+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#REMOVE DAYS FROM  EXISTING DATE\n",
    "df_rem = df.withColumn(\"SUB_2_days\",date_sub(\"StartDate\",2))\n",
    "df_rem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3138c5a-59e0-4c9e-a6df-c2317659ccdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+------------+---------------+\n|S_no| StartDate|   EndDate|add_2_months|Remove_2_months|\n+----+----------+----------+------------+---------------+\n|   1|2023-06-07|12/31/2023|  2023-08-07|     2023-04-07|\n|   2|2022-02-01|11/30/2022|  2022-04-01|     2021-12-01|\n|   3|2023-12-31|11/30/2023|  2024-02-29|     2023-10-31|\n|   4|2024-01-01|01/31/2024|  2024-03-01|     2023-11-01|\n|   5|2024-02-01|02/28/2024|  2024-04-01|     2023-12-01|\n|   6|2025-01-07|06/30/2025|  2025-03-07|     2024-11-07|\n+----+----------+----------+------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#ADD/REMOVE moths to existing date\n",
    "# FOR YEAR, QUARTER,HALF YEAR use multiples of monts as we dont have a direct dataadd funcion for year, quarter,month\n",
    "#2 TEAR = 24 MONTHS, 2 QUARTERS = 6 MONTHS\n",
    "\n",
    "df_add2 = df.withColumn(\"add_2_months\",add_months(\"StartDate\",2))\\\n",
    "            .withColumn(\"Remove_2_months\",add_months(\"StartDate\",-2))\n",
    "df_add2.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7420a3ca-1a9d-433d-b47f-6f7d447f30d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------------+-------------------+-------------------+\n|S_no| StartDate|   EndDate|        ADD_2_years|     ADD_3_Quarters|        ADD_5_weeks|\n+----+----------+----------+-------------------+-------------------+-------------------+\n|   1|2023-06-07|12/31/2023|2025-06-07 00:00:00|2024-03-07 00:00:00|2023-07-12 00:00:00|\n|   2|2022-02-01|11/30/2022|2024-02-01 00:00:00|2022-11-01 00:00:00|2022-03-08 00:00:00|\n|   3|2023-12-31|11/30/2023|2025-12-31 00:00:00|2024-09-30 00:00:00|2024-02-04 00:00:00|\n|   4|2024-01-01|01/31/2024|2026-01-01 00:00:00|2024-10-01 00:00:00|2024-02-05 00:00:00|\n|   5|2024-02-01|02/28/2024|2026-02-01 00:00:00|2024-11-01 00:00:00|2024-03-07 00:00:00|\n|   6|2025-01-07|06/30/2025|2027-01-07 00:00:00|2025-10-07 00:00:00|2025-02-11 00:00:00|\n+----+----------+----------+-------------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Alternate option to use date add for year, quarter,week\n",
    "# use EXPRESSION LANUAGE\n",
    "df_add3 = df.withColumn(\"ADD_2_years\",expr(\"DATEADD(year,2,StartDate)\"))\\\n",
    "            .withColumn(\"ADD_3_Quarters\",expr(\"DATEADD(quarter,3,StartDate)\"))\\\n",
    "            .withColumn(\"ADD_5_weeks\",expr(\"DATEADD(WEEK,5,sTARTdATE)\"))    \n",
    "df_add3.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a754dd7-e8df-40ad-99c7-ca6ea4fbeee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+\n|S_no| StartDate|   EndDate|  Last_Day|\n+----+----------+----------+----------+\n|   1|2023-06-07|12/31/2023|2023-06-30|\n|   2|2022-02-01|11/30/2022|2022-02-28|\n|   3|2023-12-31|11/30/2023|2023-12-31|\n|   4|2024-01-01|01/31/2024|2024-01-31|\n|   5|2024-02-01|02/28/2024|2024-02-29|\n|   6|2025-01-07|06/30/2025|2025-01-31|\n+----+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#LAST DAY OF MONTH\n",
    "df_lst = df.withColumn(\"Last_Day\",last_day(\"StartDate\"))\n",
    "df_lst.show()                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf51d35a-ed6b-47ef-8f00-11b827f239d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-----------+--------------------+\n|S_no| StartDate|   EndDate|CurrentDate|    CUrrentTimeStamp|\n+----+----------+----------+-----------+--------------------+\n|   1|2023-06-07|12/31/2023| 2025-06-24|2025-06-24 16:47:...|\n|   2|2022-02-01|11/30/2022| 2025-06-24|2025-06-24 16:47:...|\n|   3|2023-12-31|11/30/2023| 2025-06-24|2025-06-24 16:47:...|\n|   4|2024-01-01|01/31/2024| 2025-06-24|2025-06-24 16:47:...|\n|   5|2024-02-01|02/28/2024| 2025-06-24|2025-06-24 16:47:...|\n|   6|2025-01-07|06/30/2025| 2025-06-24|2025-06-24 16:47:...|\n+----+----------+----------+-----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Currentdate will give the output as date only\n",
    "#Currenttimestamp will return both date and time in UTC timezone\n",
    "\n",
    "df_curr = df.withColumn(\"CurrentDate\",current_date())\\\n",
    "            .withColumn(\"CUrrentTimeStamp\",current_timestamp())\n",
    "df_curr.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "599df9c9-e294-42ce-9996-8e8431507b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------------+-------------------+-------------------+-------------------+\n|S_no| StartDate|   EndDate|      START_OF_YEAR|     START_OF_MONTH|   START_OF_QUARTER|       START_OF_DAY|\n+----+----------+----------+-------------------+-------------------+-------------------+-------------------+\n|   1|2023-06-07|12/31/2023|2023-01-01 00:00:00|2023-06-01 00:00:00|2023-04-01 00:00:00|2023-06-07 00:00:00|\n|   2|2022-02-01|11/30/2022|2022-01-01 00:00:00|2022-02-01 00:00:00|2022-01-01 00:00:00|2022-02-01 00:00:00|\n|   3|2023-12-31|11/30/2023|2023-01-01 00:00:00|2023-12-01 00:00:00|2023-10-01 00:00:00|2023-12-31 00:00:00|\n|   4|2024-01-01|01/31/2024|2024-01-01 00:00:00|2024-01-01 00:00:00|2024-01-01 00:00:00|2024-01-01 00:00:00|\n|   5|2024-02-01|02/28/2024|2024-01-01 00:00:00|2024-02-01 00:00:00|2024-01-01 00:00:00|2024-02-01 00:00:00|\n|   6|2025-01-07|06/30/2025|2025-01-01 00:00:00|2025-01-01 00:00:00|2025-01-01 00:00:00|2025-01-07 00:00:00|\n+----+----------+----------+-------------------+-------------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# date_trunc will take you the start of the year, month,week,etc\n",
    "# syntax: date_trunc('interval','datecol')\n",
    "# acceptable paameters in datetrunc function\n",
    "#'year','yyyy','yy','month','mon','mm','day','dd','hour','minute','second','week','quater'\n",
    "df_trunc = df.withColumn(\"START_OF_YEAR\",date_trunc('yyyy','StartDate'))\\\n",
    "             .withColumn(\"START_OF_MONTH\",date_trunc('month','startDate'))\\\n",
    "             .withColumn(\"START_OF_QUARTER\",date_trunc('quarter','startDate'))\\\n",
    "             .withColumn(\"START_OF_DAY\",date_trunc('day','startDate'))                         \n",
    "df_trunc.show()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d5cd08-e0a1-4d2c-b8fb-21670e0d890d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------------+-------------------+-------------------+------------+\n|S_no| StartDate|   EndDate|      START_OF_YEAR|     START_OF_MONTH|   START_OF_QUARTER|Start_of_Day|\n+----+----------+----------+-------------------+-------------------+-------------------+------------+\n|   1|2023-06-07|12/31/2023|2023-01-01 00:00:00|2023-06-01 00:00:00|2023-04-01 00:00:00|  2023-06-07|\n|   2|2022-02-01|11/30/2022|2022-01-01 00:00:00|2022-02-01 00:00:00|2022-01-01 00:00:00|  2022-02-01|\n|   3|2023-12-31|11/30/2023|2023-01-01 00:00:00|2023-12-01 00:00:00|2023-10-01 00:00:00|  2023-12-31|\n|   4|2024-01-01|01/31/2024|2024-01-01 00:00:00|2024-01-01 00:00:00|2024-01-01 00:00:00|  2024-01-01|\n|   5|2024-02-01|02/28/2024|2024-01-01 00:00:00|2024-02-01 00:00:00|2024-01-01 00:00:00|  2024-02-01|\n|   6|2025-01-07|06/30/2025|2025-01-01 00:00:00|2025-01-01 00:00:00|2025-01-01 00:00:00|  2025-01-07|\n+----+----------+----------+-------------------+-------------------+-------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#DATEformat helps you in changing he format of the date\n",
    "#syntax: date_format('datecolumn','dateformat')\n",
    "# Accepted string in Date Format\n",
    "# yyyy,yy,MMMM,MMM,MM,dd,hh,mm,ss\n",
    "\n",
    "# remove timestamp from above df_trunc dataframe from start_of_day\n",
    "\n",
    "df_fmt = df_trunc.withColumn(\"Start_of_Day\",date_format('START_OF_DAY','yyyy-MM-dd'))\n",
    "df_fmt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3546bfdf-c8b4-473d-ac05-2c5aa6440db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+------------+\n|S_no| StartDate|   EndDate|MM-yy format|\n+----+----------+----------+------------+\n|   1|2023-06-07|12/31/2023|      Jun-23|\n|   2|2022-02-01|11/30/2022|      Feb-22|\n|   3|2023-12-31|11/30/2023|      Dec-23|\n|   4|2024-01-01|01/31/2024|      Jan-24|\n|   5|2024-02-01|02/28/2024|      Feb-24|\n|   6|2025-01-07|06/30/2025|      Jan-25|\n+----+----------+----------+------------+\n\n+----+----------+----------+------------+\n|S_no| StartDate|   EndDate|MM-yy format|\n+----+----------+----------+------------+\n|   1|2023-06-07|12/31/2023|     June-23|\n|   2|2022-02-01|11/30/2022| February-22|\n|   3|2023-12-31|11/30/2023| December-23|\n|   4|2024-01-01|01/31/2024|  January-24|\n|   5|2024-02-01|02/28/2024| February-24|\n|   6|2025-01-07|06/30/2025|  January-25|\n+----+----------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Get the date format in \"jUN-25\"\n",
    "df_fmt2 = df.withColumn(\"MM-yy format\",date_format(\"StartDate\",'MMM-yy'))\n",
    "df_fmt2.show()\n",
    "#Get the date format in \"JUNE-25\"\n",
    "df_fmt3 = df.withColumn(\"MM-yy format\",date_format(\"StartDate\",'MMMM-yy'))\n",
    "df_fmt3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d4bbca-4923-4819-ad52-62466dabf3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+--------------------+----+-------+-------+\n|S_no| StartDate|   EndDate|  Current_Time_Stamp|HOUR|Minutes|Seconds|\n+----+----------+----------+--------------------+----+-------+-------+\n|   1|2023-06-07|12/31/2023|2025-06-26 15:56:...|  03|     56|     30|\n|   2|2022-02-01|11/30/2022|2025-06-26 15:56:...|  03|     56|     30|\n|   3|2023-12-31|11/30/2023|2025-06-26 15:56:...|  03|     56|     30|\n|   4|2024-01-01|01/31/2024|2025-06-26 15:56:...|  03|     56|     30|\n|   5|2024-02-01|02/28/2024|2025-06-26 15:56:...|  03|     56|     30|\n|   6|2025-01-07|06/30/2025|2025-06-26 15:56:...|  03|     56|     30|\n+----+----------+----------+--------------------+----+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#Get the hour, minutes,seconds from a date\n",
    "df2 = df.withColumn(\"Current_Time_Stamp\",current_timestamp())\n",
    "df3 = df2.withColumn(\"HOUR\",date_format(\"CURRENT_TIME_STAMP\",'hh'))\\\n",
    "         .withColumn(\"Minutes\",date_format(\"CURRENT_TIME_STAMP\",'mm'))\\\n",
    "         .withColumn(\"Seconds\",date_format(\"CURRENT_TIME_STAMP\",'ss'))    \n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5940c5c-f6e6-4982-8052-8ba6eb1c25b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+--------------------+---------+\n|S_no| StartDate|   EndDate|  Current_Time_Stamp|Diff_year|\n+----+----------+----------+--------------------+---------+\n|   1|2023-06-07|12/31/2023|2025-06-26 15:59:...|      750|\n|   2|2022-02-01|11/30/2022|2025-06-26 15:59:...|     1241|\n|   3|2023-12-31|11/30/2023|2025-06-26 15:59:...|      543|\n|   4|2024-01-01|01/31/2024|2025-06-26 15:59:...|      542|\n|   5|2024-02-01|02/28/2024|2025-06-26 15:59:...|      511|\n|   6|2025-01-07|06/30/2025|2025-06-26 15:59:...|      170|\n+----+----------+----------+--------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#datediff function will return the difference in the form of no of days\n",
    "df2 = df.withColumn(\"Current_Time_Stamp\",current_timestamp())\n",
    "df_diff = df2.withColumn(\"Diff_year\",datediff(\"Current_Time_Stamp\",\"StartDate\"))\n",
    "df_diff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a76a960-7871-4350-9630-4a4231864834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------------+--------------------+\n|S_no| StartDate|   EndDate|       StartofToday|  current_Time_Stamp|\n+----+----------+----------+-------------------+--------------------+\n|   1|2023-06-07|12/31/2023|2025-06-26 00:00:00|2025-06-26 16:08:...|\n|   2|2022-02-01|11/30/2022|2025-06-26 00:00:00|2025-06-26 16:08:...|\n|   3|2023-12-31|11/30/2023|2025-06-26 00:00:00|2025-06-26 16:08:...|\n|   4|2024-01-01|01/31/2024|2025-06-26 00:00:00|2025-06-26 16:08:...|\n|   5|2024-02-01|02/28/2024|2025-06-26 00:00:00|2025-06-26 16:08:...|\n|   6|2025-01-07|06/30/2025|2025-06-26 00:00:00|2025-06-26 16:08:...|\n+----+----------+----------+-------------------+--------------------+\n\nroot\n |-- S_no: long (nullable = true)\n |-- StartDate: string (nullable = true)\n |-- EndDate: string (nullable = true)\n |-- StartofToday: timestamp (nullable = true)\n |-- current_Time_Stamp: timestamp (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "#get the difference in time B/W start of todays date and current time\n",
    "df2 = df.withColumn(\"StartofToday\",date_trunc('day',current_timestamp()))\\\n",
    "        .withColumn(\"current_Time_Stamp\",current_timestamp())\n",
    "df2.show()    \n",
    "df2.printSchema()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9290edf-db11-4e18-adfd-5471cd76aa38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------------+--------------------+-----+\n|S_no| StartDate|   EndDate|       StartofToday|  current_Time_Stamp| test|\n+----+----------+----------+-------------------+--------------------+-----+\n|   1|2023-06-07|12/31/2023|2025-06-26 00:00:00|2025-06-26 16:29:...|59387|\n|   2|2022-02-01|11/30/2022|2025-06-26 00:00:00|2025-06-26 16:29:...|59387|\n|   3|2023-12-31|11/30/2023|2025-06-26 00:00:00|2025-06-26 16:29:...|59387|\n|   4|2024-01-01|01/31/2024|2025-06-26 00:00:00|2025-06-26 16:29:...|59387|\n|   5|2024-02-01|02/28/2024|2025-06-26 00:00:00|2025-06-26 16:29:...|59387|\n|   6|2025-01-07|06/30/2025|2025-06-26 00:00:00|2025-06-26 16:29:...|59387|\n+----+----------+----------+-------------------+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.withColumn(\"test\",(unix_timestamp(\"current_Time_Stamp\") - unix_timestamp(\"StartofToday\")))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5e6067-3d76-4002-9da6-60454b6f5d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Start Date': datetime.datetime(2024, 6, 26, 8, 2), 'End Date': datetime.datetime(2024, 6, 26, 22, 3)}, {'Start Date': datetime.datetime(2024, 5, 6, 8, 2), 'End Date': datetime.datetime(2024, 5, 6, 9, 3)}]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#create a lsit with Start date and End Date\n",
    "\n",
    "date_list = [\n",
    "    {\"Start Date\": datetime(2024,6,26,8,2,0), \"End Date\": datetime(2024,6,26,22,3,0)},\n",
    "    {\"Start Date\": datetime(2024,5,6,8,2,0), \"End Date\": datetime(2024,5,6,9,3,0)}\n",
    "\n",
    "]\n",
    "print(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2a1c39-af3a-47d3-a38c-75327abdbd5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+\n|           End Date|         Start Date|DateDiff|\n+-------------------+-------------------+--------+\n|2024-06-26 22:03:00|2024-06-26 08:02:00|   50460|\n|2024-05-06 09:03:00|2024-05-06 08:02:00|    3660|\n+-------------------+-------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DateDiff\").getOrCreate()\n",
    "\n",
    "# Create a dataFrame from the date list\n",
    "date_df = spark.createDataFrame(date_list)\n",
    "\n",
    "# Calculate the difference in seconds B/W start date and End Date\n",
    "\n",
    "date_df = date_df.withColumn(\"DateDiff\",(unix_timestamp(col(\"End Date\")) - unix_timestamp(col(\"Start Date\"))))\n",
    "\n",
    "# show the results\n",
    "date_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-06-24 21:20:48",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}