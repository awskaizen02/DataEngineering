{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64de1cc-0945-4d20-9638-3c48af15e4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+\n|firstname|lastname|country|state|  sal|\n+---------+--------+-------+-----+-----+\n|     raja|  pushpa|    USA|     |30000|\n|    priya|  pushpa|    USA|     |29900|\n|  Karthik|    Subu|    USA|   CA| 6000|\n|    James|   Smith|    USA|   FL|20000|\n|   Martin|   Jones|    USA|   CA| 3000|\n|      Sam|Anderson|     UK|  LND| 8000|\n|    Maria| Patrick|     UK|  MCR| 7000|\n|    Robet|   Bevon|     UK|  LND| 3500|\n|    Maria|Anderson|     UK|  MCR| 3000|\n+---------+--------+-------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe from a static list\n",
    "# syntax: spark.createDataFrame(data = <list> , schema = <shemalist>)\n",
    "staticlist = [(\" raja\", \"pushpa\", \"USA\",\"\",30000),\n",
    "            (\" priya\", \"pushpa\", \"USA\",\"\",29900),\n",
    "            (\" Karthik\", \"Subu\", \"USA\",\"CA\",6000),\n",
    "            (\" James\", \"Smith\", \"USA\",\"FL\",20000),\n",
    "            (\"Martin\", \"Jones\", \"USA\",\"CA\",3000),\n",
    "            (\"Sam\", \"Anderson\", \"UK\",\"LND\",8000),\n",
    "            (\"Maria\", \"Patrick\", \"UK\",\"MCR\",7000),\n",
    "            (\"Robet\", \"Bevon\", \"UK\",\"LND\",3500),\n",
    "            (\"Maria\", \"Anderson\", \"UK\",\"MCR\",3000)\n",
    "            ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\",\"sal\"]            \n",
    "df = spark.createDataFrame( data = staticlist, schema = columns)\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a1eadb5-3bf1-44f1-bca9-e0a3bfbd41b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: pyspark.sql.dataframe.DataFrame"
     ]
    }
   ],
   "source": [
    "#Check the datatype of any variable\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c182b5-38d5-4ef2-bd9b-24be45a38b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n |-- sal: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#check the schema of the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27cc79bc-267d-45cd-bda2-bf13e7acbde1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# crea a new schema for a datafram using the structureType and structField\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "StructSchema = StructType([\n",
    "                            StructField('FirstName',StringType(),False),\n",
    "                            StructField('LastName',StringType(),True),\n",
    "                            StructField('Country',StringType(),False),\n",
    "                            StructField('State',StringType(),False),\n",
    "                            StructField('Salary',StringType(),False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ece336e-3d00-40ec-82ce-71a5594bf85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+------+\n|FirstName|LastName|Country|State|Salary|\n+---------+--------+-------+-----+------+\n|     raja|  pushpa|    USA|     | 30000|\n|    priya|  pushpa|    USA|     | 29900|\n|  Karthik|    Subu|    USA|   CA|  6000|\n|    James|   Smith|    USA|   FL| 20000|\n|   Martin|   Jones|    USA|   CA|  3000|\n|      Sam|Anderson|     UK|  LND|  8000|\n|    Maria| Patrick|     UK|  MCR|  7000|\n|    Robet|   Bevon|     UK|  LND|  3500|\n|    Maria|Anderson|     UK|  MCR|  3000|\n+---------+--------+-------+-----+------+\n\nroot\n |-- FirstName: string (nullable = false)\n |-- LastName: string (nullable = true)\n |-- Country: string (nullable = false)\n |-- State: string (nullable = false)\n |-- Salary: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "# creat the datafram using the new struct schem\n",
    "\n",
    "df = spark.createDataFrame(data = staticlist, schema = StructSchema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d63655f4-2090-4af1-a262-f296abbd85b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# most used pyspark libaries\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c88ef1e-d1a4-44ed-ac8a-0d4b3106755e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|firstname|LASTNAME|\n+---------+--------+\n|     raja|  pushpa|\n|    priya|  pushpa|\n|  Karthik|    Subu|\n|    James|   Smith|\n|   Martin|   Jones|\n|      Sam|Anderson|\n|    Maria| Patrick|\n|    Robet|   Bevon|\n|    Maria|Anderson|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# select specific columns from dataframe using the \"column name\" SELECTION\n",
    "# Using name in inverted columns is not case sensitive\n",
    "df1 = df.select(\"FirstName\",\"LASTNAME\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8532063a-9515-4054-80ec-33b2baf3c4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|FirstName|LastName|\n+---------+--------+\n|     raja|  pushpa|\n|    priya|  pushpa|\n|  Karthik|    Subu|\n|    James|   Smith|\n|   Martin|   Jones|\n|      Sam|Anderson|\n|    Maria| Patrick|\n|    Robet|   Bevon|\n|    Maria|Anderson|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#option2 using the datafram method\n",
    "#using this method the column names should be case sensitive\n",
    "df1 = df.select(df.FirstName, df.LastName)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558f003c-b2b8-4837-b29a-288544c7bf3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|FIRSTNAME|LASTNAME|\n+---------+--------+\n|     raja|  pushpa|\n|    priya|  pushpa|\n|  Karthik|    Subu|\n|    James|   Smith|\n|   Martin|   Jones|\n|      Sam|Anderson|\n|    Maria| Patrick|\n|    Robet|   Bevon|\n|    Maria|Anderson|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#option3 using the datafram method 2\n",
    "#using this method the column names are not case sensitive\n",
    "df1 = df.select(df[\"FIRSTNAME\"],df[\"LASTNAME\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d795f407-dc9f-4da8-9d73-69c1d56d852a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|FIRSTNAME|LASTNAME|\n+---------+--------+\n|     raja|  pushpa|\n|    priya|  pushpa|\n|  Karthik|    Subu|\n|    James|   Smith|\n|   Martin|   Jones|\n|      Sam|Anderson|\n|    Maria| Patrick|\n|    Robet|   Bevon|\n|    Maria|Anderson|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(df[\"FIRSTNAME\"],df[\"LASTNAME\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614e96be-e726-41fe-a931-81430afd338c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|firstname|LASTNAME|\n+---------+--------+\n|     raja|  pushpa|\n|    priya|  pushpa|\n|  Karthik|    Subu|\n|    James|   Smith|\n|   Martin|   Jones|\n|      Sam|Anderson|\n|    Maria| Patrick|\n|    Robet|   Bevon|\n|    Maria|Anderson|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# select the columns ased on column method\n",
    "# column name is not case sensitive in col method\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df2 = df.select(col(\"firstname\"),col(\"LASTNAME\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a459cb62-2f10-4db5-bb6f-19b2601df685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2813619087871657>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df1 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect(df\u001B[38;5;241m.\u001B[39mfirstname, df\u001B[38;5;241m.\u001B[39mlastname)\n",
       "\u001B[1;32m      2\u001B[0m df1\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2964\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   2934\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n",
       "\u001B[1;32m   2935\u001B[0m \n",
       "\u001B[1;32m   2936\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2961\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n",
       "\u001B[1;32m   2962\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   2963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n",
       "\u001B[0;32m-> 2964\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n",
       "\u001B[1;32m   2965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n",
       "\u001B[1;32m   2966\u001B[0m     )\n",
       "\u001B[1;32m   2967\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n",
       "\u001B[1;32m   2968\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'firstname'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-2813619087871657>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df1 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect(df\u001B[38;5;241m.\u001B[39mfirstname, df\u001B[38;5;241m.\u001B[39mlastname)\n\u001B[1;32m      2\u001B[0m df1\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2964\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   2934\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n\u001B[1;32m   2935\u001B[0m \n\u001B[1;32m   2936\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2961\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n\u001B[1;32m   2962\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m-> 2964\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m   2965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n\u001B[1;32m   2966\u001B[0m     )\n\u001B[1;32m   2967\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n\u001B[1;32m   2968\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n\n\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'firstname'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'DataFrame' object has no attribute 'firstname'",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = df.select(df.firstname, df.lastname)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c3e333-719d-487b-bd4c-cc377eaf6ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|FirstName|LastName|\n+---------+--------+\n|     raja|  pushpa|\n|    priya|  pushpa|\n|  Karthik|    Subu|\n|    James|   Smith|\n|   Martin|   Jones|\n|      Sam|Anderson|\n|    Maria| Patrick|\n|    Robet|   Bevon|\n|    Maria|Anderson|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# select the column names based on wildcard name\n",
    "df1 = df.select(df.colRegex(\"`.*name*`\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a6f1ad-1ca9-4e89-9c6d-07d0933db8bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- FirstName: string (nullable = false)\n |-- LastName: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44aa8fe3-fafc-4797-8c02-7d3634bef015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = false)\n |-- lastname: string (nullable = true)\n |-- Country: string (nullable = false)\n |-- State: string (nullable = false)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_cast1 = df.select(\"firstname\",\"lastname\",\"Country\",\"State\",\n",
    "                     df.Salary.cast(IntegerType()))\n",
    "df_cast1.printSchema()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692b4072-daf4-4d74-9343-703f8c4a182c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- FirstName: string (nullable = false)\n |-- LastName: string (nullable = true)\n |-- Country: string (nullable = false)\n |-- State: string (nullable = false)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#option2: using the whildColumn method\n",
    "\n",
    "df_cast2 = df.withColumn(\"Salary\",df.Salary.cast(IntegerType()))\n",
    "df_cast2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5cea99-76f5-46b4-9e4c-59d2e20c2f89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+------+\n|FirstName|LastName|Country|State|Salary|\n+---------+--------+-------+-----+------+\n|     raja|  pushpa|   null|     | 30000|\n|    priya|  pushpa|   null|     | 29900|\n|  Karthik|    Subu|   null|   CA|  6000|\n|    James|   Smith|   null|   FL| 20000|\n|   Martin|   Jones|   null|   CA|  3000|\n|      Sam|Anderson|   null|  LND|  8000|\n|    Maria| Patrick|   null|  MCR|  7000|\n|    Robet|   Bevon|   null|  LND|  3500|\n|    Maria|Anderson|   null|  MCR|  3000|\n+---------+--------+-------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_cast2 = df.withColumn(\"Country\",df.Country.cast(IntegerType()))\n",
    "df_cast2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7badd980-64e5-46f4-8dcf-2f3340e08fc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+------+----------------+\n|FirstName|LastName|Country|State|Salary|Salary_Increment|\n+---------+--------+-------+-----+------+----------------+\n|     raja|  pushpa|    USA|     | 30000|         60000.0|\n|    priya|  pushpa|    USA|     | 29900|         59800.0|\n|  Karthik|    Subu|    USA|   CA|  6000|         12000.0|\n|    James|   Smith|    USA|   FL| 20000|         40000.0|\n|   Martin|   Jones|    USA|   CA|  3000|          6000.0|\n|      Sam|Anderson|     UK|  LND|  8000|         16000.0|\n|    Maria| Patrick|     UK|  MCR|  7000|         14000.0|\n|    Robet|   Bevon|     UK|  LND|  3500|          7000.0|\n|    Maria|Anderson|     UK|  MCR|  3000|          6000.0|\n+---------+--------+-------+-----+------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Adding a new calculated columns using the withColumn method\n",
    "# add a new salary column with double the current salary\n",
    "df1 = df.withColumn(\"Salary_Increment\",df.Salary*2)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8217a7-9469-442b-bb57-7c0641554b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+------+----------------+----------------+\n|FirstName|LastName|Country|State|Salary|Salary_Increment|        FullName|\n+---------+--------+-------+-----+------+----------------+----------------+\n|     raja|  pushpa|    USA|     | 30000|         60000.0|   raja   pushpa|\n|    priya|  pushpa|    USA|     | 29900|         59800.0|  priya   pushpa|\n|  Karthik|    Subu|    USA|   CA|  6000|         12000.0|  Karthik   Subu|\n|    James|   Smith|    USA|   FL| 20000|         40000.0|   James   Smith|\n|   Martin|   Jones|    USA|   CA|  3000|          6000.0|  Martin   Jones|\n|      Sam|Anderson|     UK|  LND|  8000|         16000.0|  Sam   Anderson|\n|    Maria| Patrick|     UK|  MCR|  7000|         14000.0| Maria   Patrick|\n|    Robet|   Bevon|     UK|  LND|  3500|          7000.0|   Robet   Bevon|\n|    Maria|Anderson|     UK|  MCR|  3000|          6000.0|Maria   Anderson|\n+---------+--------+-------+-----+------+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# add a new calculated fll name , concatenate first and last name\n",
    "df1 = df.withColumn(\"FullName\",concat(df.FirstName,lit(\"   \"), df.LastName))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08bdc3b4-6cf5-4ef2-9e99-65dd09ca9ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+------+----------------+----------+\n|FirstName|LastName|Country|State|Salary|Salary_Increment|ReportNmae|\n+---------+--------+-------+-----+------+----------------+----------+\n|     raja|  pushpa|    USA|     | 30000|         60000.0| empdetail|\n|    priya|  pushpa|    USA|     | 29900|         59800.0| empdetail|\n|  Karthik|    Subu|    USA|   CA|  6000|         12000.0| empdetail|\n|    James|   Smith|    USA|   FL| 20000|         40000.0| empdetail|\n|   Martin|   Jones|    USA|   CA|  3000|          6000.0| empdetail|\n|      Sam|Anderson|     UK|  LND|  8000|         16000.0| empdetail|\n|    Maria| Patrick|     UK|  MCR|  7000|         14000.0| empdetail|\n|    Robet|   Bevon|     UK|  LND|  3500|          7000.0| empdetail|\n|    Maria|Anderson|     UK|  MCR|  3000|          6000.0| empdetail|\n+---------+--------+-------+-----+------+----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# add a static column called Reportname and values should be \"empdetail\"\n",
    "df1 = df.withColumn(\"ReportNmae\",lit(\"empdetail\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0babd9a-88f1-4221-a431-8cc997da9007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+---+------+\n|  F_NAME|  L_NAME| ST|cnt|Salary|\n+--------+--------+---+---+------+\n|    raja|  pushpa|   |USA| 30000|\n|   priya|  pushpa|   |USA| 29900|\n| Karthik|    Subu| CA|USA|  6000|\n|   James|   Smith| FL|USA| 20000|\n|  Martin|   Jones| CA|USA|  3000|\n|     Sam|Anderson|LND| UK|  8000|\n|   Maria| Patrick|MCR| UK|  7000|\n|   Robet|   Bevon|LND| UK|  3500|\n|   Maria|Anderson|MCR| UK|  3000|\n+--------+--------+---+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Rename a column\n",
    "# option1: using the select method\n",
    "df1 = df.select(df.FirstName.alias(\"F_NAME\"),\n",
    "               df.LastName.alias(\"L_NAME\"),\n",
    "               df.State.alias(\"ST\"),\n",
    "               df.Country.alias(\"cnt\"), df.Salary)\n",
    "df1.show()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f266194a-7f48-4d09-a441-5e28d8a00ba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+-------+------+\n|  F_NAME|  L_NAME|State|Country|Salary|\n+--------+--------+-----+-------+------+\n|    raja|  pushpa|     |    USA| 30000|\n|   priya|  pushpa|     |    USA| 29900|\n| Karthik|    Subu|   CA|    USA|  6000|\n|   James|   Smith|   FL|    USA| 20000|\n|  Martin|   Jones|   CA|    USA|  3000|\n|     Sam|Anderson|  LND|     UK|  8000|\n|   Maria| Patrick|  MCR|     UK|  7000|\n|   Robet|   Bevon|  LND|     UK|  3500|\n|   Maria|Anderson|  MCR|     UK|  3000|\n+--------+--------+-----+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(df.FirstName.alias(\"F_NAME\"),\n",
    "               df.LastName.alias(\"L_NAME\"),\n",
    "               df.State,df.Country, df.Salary)\n",
    "df1.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b38759dc-0236-4b90-8b76-d3fc95fd5663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+-----+------+----------------+\n|  F_NAME|  L_NAME|Country|State|Salary|Salary_Increment|\n+--------+--------+-------+-----+------+----------------+\n|    raja|  pushpa|    USA|     | 30000|         60000.0|\n|   priya|  pushpa|    USA|     | 29900|         59800.0|\n| Karthik|    Subu|    USA|   CA|  6000|         12000.0|\n|   James|   Smith|    USA|   FL| 20000|         40000.0|\n|  Martin|   Jones|    USA|   CA|  3000|          6000.0|\n|     Sam|Anderson|     UK|  LND|  8000|         16000.0|\n|   Maria| Patrick|     UK|  MCR|  7000|         14000.0|\n|   Robet|   Bevon|     UK|  LND|  3500|          7000.0|\n|   Maria|Anderson|     UK|  MCR|  3000|          6000.0|\n+--------+--------+-------+-----+------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Option2 using the with ColumRenamed method\n",
    "df1 = df.withColumnRenamed(\"FirstName\",\"F_NAME\")\\\n",
    "        .withColumnRenamed(\"LastName\",\"L_NAME\")\n",
    "df1.show()        "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-06-19 08:19:49",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}